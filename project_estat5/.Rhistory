gbm.ROC <- roc(predictor=gbm.probs$PS,
response=testData$Class,
levels=rev(levels(testData$Class)))
gbm.ROC$auc
#Area under the curve: 0.8731
plot(gbm.ROC,main="GBM ROC")
# Plot the propability of poor segmentation
histogram(~gbm.probs$PS|testData$Class,xlab="Probability of Poor Segmentation")
##----------------------------------------------
## XGBOOST
# Some stackexchange guidance for xgboost
# http://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees
# Set up for parallel procerssing
set.seed(1951)
registerDoParallel(4,cores=4)
getDoParWorkers()
# Train xgboost
#Não funcionou com os dados que vieram no código
#xgb.grid <- expand.grid(nrounds = 500, #the maximum number of iterations
#                        eta = c(0.01,0.1), # shrinkage
#                        max_depth = c(2,6,10))
xgb.grid <- expand.grid(eta = c(0.05, 0.075, 0.1),
nrounds = c(50, 75, 100),
max_depth = 6:8,
min_child_weight = c(2.0, 2.25, 2.5),
colsample_bytree = c(0.3, 0.4, 0.5),
gamma = 0,
subsample = 1)
xgb.tune <-train(x=trainX,y=trainData$Class,
method="xgbTree",
metric="ROC",
trControl=ctrl,
tuneGrid=xgb.grid)
stopImplicitCluster()
xgb.tune$bestTune
plot(xgb.tune)  		# Plot the performance of the training models
res <- xgb.tune$results
res
### xgboostModel Predictions and Performance
# Make predictions using the test data set
xgb.pred <- predict(xgb.tune,testX)
#Look at the confusion matrix
confusionMatrix(xgb.pred,testData$Class)
#Draw the ROC curve
xgb.probs <- predict(xgb.tune,testX,type="prob")
#head(xgb.probs)
xgb.ROC <- roc(predictor=xgb.probs$PS,
response=testData$Class,
levels=rev(levels(testData$Class)))
xgb.ROC$auc
# Area under the curve: 0.8857
plot(xgb.ROC,main="xgboost ROC")
# Plot the propability of poor segmentation
histogram(~xgb.probs$PS|testData$Class,xlab="Probability of Poor Segmentation")
# Comparing Multiple Models
# Having set the same seed before running gbm.tune and xgb.tune
# we have generated paired samples and are in a position to compare models
# using a resampling technique.
# (See Hothorn at al, "The design and analysis of benchmark experiments
# -Journal of Computational and Graphical Statistics (2005) vol 14 (3)
# pp 675-699)
rValues <- resamples(list(xgb=xgb.tune,gbm=gbm.tune))
rValues$values
summary(rValues)
bwplot(rValues,metric="ROC",main="GBM vs xgboost")	# boxplot
dotplot(rValues,metric="ROC",main="GBM vs xgboost")	# dotplot
#splom(rValues,metric="ROC")
#install.packages(c('caret', 'skimr', 'RANN', 'randomForest', 'fastAdaboost', 'gbm', 'xgboost', 'caretEnsemble', 'C50', 'earth'))
# Load the caret package
library(caret)
# Import dataset
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Caso o arquivo original saia do ar
#saveRDS(orange, 'Practical_Guide.rds')
#teste <- readRDS('Practical_Guide.rds')
# Structure of the dataframe
str(orange)
# See top 6 rows and 10 columns
head(orange[, 1:10])
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = factor(trainData$Purchase)
library(skimr)
skim(trainData)
# Create the knn imputation model on the training data
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
# Use the imputation model to predict the values of missing data points
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
# One-Hot Encoding
# Creating dummy variables is converting a categorical variable to as many binary variables as here are categories.
dummies_model <- dummyVars(Purchase ~ ., data=trainData)
# Create the dummy variables using predict. The Y variable (Purchase) will not be present in trainData_mat.
trainData_mat <- predict(dummies_model, newdata = trainData)
# # Convert to dataframe
trainData <- data.frame(trainData_mat)
# # See the structure of the new dataset
str(trainData)
preProcess_range_model <- preProcess(trainData, method='range')
trainData <- predict(preProcess_range_model, newdata = trainData)
# Append the Y variable
trainData$Purchase <- y
apply(trainData[, 1:10], 2, FUN=function(x){c('min'=min(x), 'max'=max(x))})
# Só funciona com y sendo fator, do contrário dá erro, ajustei o código
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
# featurePlot tb faz pairs, ellipse, density, scatter
featurePlot(x = trainData[, 1:18],
y = trainData$Purchase,
plot = "density",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=trainData[, 1:18], y=trainData$Purchase,
sizes = subsets,
rfeControl = ctrl)
lmProfile
# See available algorithms in caret
modelnames <- paste(names(getModelInfo()), collapse=',  ')
modelnames
# parâmetros do modelo
modelLookup('earth')
# Set the seed for reproducibility
set.seed(100)
# Train the model using randomForest and predict on the training data itself.
model_mars = train(Purchase ~ ., data=trainData, method='earth')
fitted <- predict(model_mars)
model_mars
plot(model_mars, main="Model Accuracies with MARS")
varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS")
# Step 1: Impute missing values
testData2 <- predict(preProcess_missingdata_model, testData)
# Step 2: Create one-hot encodings (dummy variables)
testData3 <- predict(dummies_model, testData2)
# Step 3: Transform the features to range between 0 and 1
testData4 <- predict(preProcess_range_model, testData3)
# View
head(testData4[, 1:10])
# Predict on testData
predicted <- predict(model_mars, testData4)
head(predicted)
# Compute the confusion matrix
confusionMatrix(reference = factor(testData$Purchase), data = predicted, mode='everything', positive='MM')
# Define the training control
fitControl <- trainControl(
method = 'cv',                   # k-fold cross validation
number = 5,                      # number of folds
savePredictions = 'final',       # saves predictions for optimal tuning parameter
classProbs = T,                  # should class probabilities be returned
summaryFunction=twoClassSummary  # results summary function
)
# Step 1: Tune hyper parameters by setting tuneLength
set.seed(100)
model_mars2 = train(Purchase ~ ., data=trainData, method='earth', tuneLength = 5, metric='ROC', trControl = fitControl)
model_mars2
# Step 2: Predict on testData and Compute the confusion matrix
predicted2 <- predict(model_mars2, testData4)
confusionMatrix(reference = factor(testData$Purchase), data = predicted2, mode='everything', positive='MM')
# Step 1: Define the tuneGrid
marsGrid <-  expand.grid(nprune = c(2, 4, 6, 8, 10),
degree = c(1, 2, 3))
# Step 2: Tune hyper parameters by setting tuneGrid
set.seed(100)
model_mars3 = train(Purchase ~ ., data=trainData, method='earth', metric='ROC', tuneGrid = marsGrid, trControl = fitControl)
model_mars3
# Step 3: Predict on testData and Compute the confusion matrix
predicted3 <- predict(model_mars3, testData4)
confusionMatrix(reference = factor(testData$Purchase), data = predicted3, mode='everything', positive='MM')
set.seed(100)
# Train the model using adaboost
model_adaboost = train(Purchase ~ ., data=trainData, method='adaboost', tuneLength=2, trControl = fitControl)
model_adaboost
set.seed(100)
# Train the model using rf
model_rf = train(Purchase ~ ., data=trainData, method='rf', tuneLength=5, trControl = fitControl)
model_rf
set.seed(100)
# Train the model using MARS
model_xgbDART = train(Purchase ~ ., data=trainData, method='xgbDART', tuneLength=5, trControl = fitControl, verbose=F)
model_xgbDART
set.seed(100)
# Train the model using MARS
model_svmRadial = train(Purchase ~ ., data=trainData, method='svmRadial', tuneLength=15, trControl = fitControl)
model_svmRadial
# Compare model performances using resample()
models_compare <- resamples(list(ADABOOST=model_adaboost, RF=model_rf, XGBDART=model_xgbDART,
MARS=model_mars3, SVM=model_svmRadial))
# Summary of the models performances
summary(models_compare)
# Não testei aqui, mas código importante
modelDifferences <- diff(models_compare)
summary(modelDifferences)
## The actual paired t-test:
modelDifferences$statistics$Accuracy
# Draw box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(models_compare, scales=scales)
library(caretEnsemble)
# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE,
summaryFunction=twoClassSummary) # Adicionei twoClassSummary
algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')
set.seed(100)
models <- caretList(Purchase ~ ., data=trainData,  metric='ROC', trControl=trainControl, methodList=algorithmList) # adicionei ROC
results <- resamples(models)
summary(results)
# Box plots to compare models
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)
# Create the trainControl
set.seed(101)
stackControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE)
# Ensemble the predictions of `models` to form a new combined prediction based on glm
stack.glm <- caretStack(models, method="glm", metric="ROC", trControl=stackControl)
print(stack.glm)
print(stack.glm)
# Predict on testData
stack_predicteds <- predict(stack.glm, newdata=testData4)
confusionMatrix(reference = factor(testData$Purchase), data = stack_predicteds, mode='everything', positive='MM')
library(caret)
# Pacote que cria as bases de teste e treinamento
# Neste exemplo k-fold = 10 com 2 repetições
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
# k-fold semm repetição
#controle_treinamento = trainControl(method = 'cv', number = 10)
# usando o pacote caret ainda
# criando um modelo usando o k-fold criando anteriormente
# definindo o método como nb = naive bayes
modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'nb')
# como usar o modelo como previsor
predict(modelo, newdata = iris)
predict(modelo, type = "prob")
# user kernel é uma opção do modelo, caret faz isso automático
print(modelo)
precisao = modelo$results$Accuracy[2]
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'rpart')
print(modelo)
precisao = modelo$results$Accuracy[2]
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'rf')
print(modelo)
precisao = modelo$results$Accuracy[2]
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'C5.0')
print(modelo)
precisao = modelo$results$Accuracy[2]
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'knn')
print(modelo)
precisao = modelo$results$Accuracy[2]
#controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
#modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'glm', family = 'binomial')
#print(modelo)
#precisao = modelo$results$Accuracy[2]
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'svmRadial')
print(modelo)
precisao = modelo$results$Accuracy[2]
controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 2)
#modelo = train(Species ~., data = iris, trControl = controle_treinamento, method = 'avNNet')
print(modelo)
precisao = modelo$results$Accuracy[2]
library(caret)
resultados10 = c()
for (i in 1:10) {
controle_treinamento = trainControl(method = 'cv', number = 10)
modelo = train(Species ~ ., data = iris, trControl = controle_treinamento, method = 'nb')
precisao = modelo$results$Accuracy[2]
print(precisao)
resultados10 = c(resultados10, precisao)
}
for (i in 1:10) {
cat(gsub('[.]', ',', resultados10[i]))
cat('\n')
}
### IMPORTANTE
# controle_treinamento = trainControl(method = 'repeatedcv', number = 10, repeats = 10)
# esse código equivale a toda a execução superior, ou seja, irá rodar tudo 10 vezes, mas ao invés de trazer o resultado de cada rodada, irá trazer já a média das 10 execuções
# caso não precise rodar os testes de friedman e nenemyi, esta seria a melhor abordagem
library(PMCMRplus)
dados <- read.csv('Cross_Validation_Manual_Dados.csv')
matriz <- as.matrix(dados)
# testa se os dados são diferentes, o valor deve ser próximo de zero
friedman.test(matriz)
# sendo diferente, precisamos testar se os resultados são estatísticamente diferentes ou
# se a diferença é um erro amostral
library('PMCMRplus')
posthoc.friedman.nemenyi.test(matriz)
posthoc.friedman.nemenyi.test(matriz)
# sendo diferente, precisamos testar se os resultados são estatísticamente diferentes ou
# se a diferença é um erro amostral
library('PMCMRplus')
posthoc.friedman.nemenyi.test(matriz)
matriz
# testa se os dados são diferentes, o valor deve ser próximo de zero
friedman.test(matriz)
posthoc.friedman.nemenyi.test(matriz)
# sendo diferente, precisamos testar se os resultados são estatísticamente diferentes ou
# se a diferença é um erro amostral
library('PMCMRplus')
# sendo diferente, precisamos testar se os resultados são estatísticamente diferentes ou
# se a diferença é um erro amostral
library('PMCMR')
# sendo diferente, precisamos testar se os resultados são estatísticamente diferentes ou
# se a diferença é um erro amostral
library('PMCMRplus')
posthoc.friedman.nemenyi.test(matriz)
# sendo diferente, precisamos testar se os resultados são estatísticamente diferentes ou
# se a diferença é um erro amostral
library('PMCMR')
install.packages('PMCMR')
library(PMCMR)
posthoc.friedman.nemenyi.test(matriz)
library(caret)
library(extraTrees)
qol<-read.csv("https://umich.instructure.com/files/481332/download?download_frd=1")
#save(qol, file="qol.Rdata", compress=TRUE)
#load("qol.Rdata")
# Limpando a base
qol<-qol[!qol$CHRONICDISEASESCORE==-9, ]
summary(qol$CHRONICDISEASESCORE)
# Vamos definir que abaixo da média é doença "menor", acima é "severe"
qol$cd<-qol$CHRONICDISEASESCORE > mean(qol$CHRONICDISEASESCORE)
qol$cd<-factor(qol$cd, levels=c(F, T), labels = c("minor_disease", "severe_disease"))
# Ordena pelo ID e depois remove
qol<-qol[order(qol$ID), ]
qol <- qol[ , -1]
set.seed(1234)
train_index <- sample(seq_len(nrow(qol)), size = 0.8*nrow(qol))
qol_train<-qol[train_index, ]
qol_test<-qol[-train_index, ]
prop.table(table(qol_train$cd))
prop.table(table(qol_test$cd))
## Run all subsequent models in parallel
library(doParallel)
numCores <- detectCores()
cl <- makePSOCKcluster(numCores-1)
registerDoParallel(cl)
# Parâmetro SavePredictions para salvar os dados e construir a matriz de confusão
# classProbs para poder usar o ROC como métrica de otimização
control <- trainControl( classProbs = TRUE, savePredictions=TRUE, method="repeatedcv",
number=10, repeats=3, summaryFunction=twoClassSummary)
system.time({
rpart.fit  <- train(cd~., data=qol_test[ , -40], method="rpart", trControl=control, metric = "ROC");
c50.fit  <- train(cd~., data=qol_test[ , -40], method="C5.0", trControl=control, metric = "ROC");
rf.fit  <- train(cd~., data=qol_test[ , -40], method="rf", trControl=control, metric = "ROC");
knn.fit <- train(cd~., data=qol_test[ , -40], method="knn", trControl=control, metric = "ROC");
svm.fit <- train(cd~., data=qol_test[ , -40], method="svmRadial", trControl=control, metric = "ROC");
et.fit  <- train(cd~., data=qol_test[ , -40], method="extraTrees", trControl=control, metric = "ROC")
})
stopCluster(cl) # close multi-core cluster
results <- resamples(list(RP = rpart.fit, C50 = c50.fit, RF=rf.fit, kNN=knn.fit, SVM=svm.fit, ET=et.fit))
# summary of model differences
summary(results)
# plot summaries
scales <- list(x=list(relation="free"), y=list(relation="free"))
bwplot(results, scales=scales)                 # Box plots of
densityplot(results, scales=scales, pch = "|") # Density plots of accuracy
dotplot(results, scales=scales)                # Dot plots of accuracy & Kappa
splom(results)      # contrast pair-wise model scatterplots of prediction accuracy
library(MLeval)
res <- evalm(list(rpart.fit,c50.fit,rf.fit, knn.fit, svm.fit, et.fit),
gnames=c('rpart','c50','rf', 'knn', 'svm', 'et'),
fsize=8)
# Se quiser achar os dados usados para calcular a curva AUC ROC
res$stdres$rpart['AUC-ROC',]
res$stdres$rpart
res$optres$rpart
res$probs
library(caret)
library(caretEnsemble)
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# Processo para preencher os dados vazios, NAs, usando kNN
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE,
summaryFunction = twoClassSummary)
algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')
set.seed(100)
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList, metric = "ROC")
# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE,
summaryFunction = twoClassSummary)
algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')
set.seed(100)
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList, metric = "ROC")
library(caret)
library(caretEnsemble)
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Create the training and test datasets
set.seed(100)
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# Processo para preencher os dados vazios, NAs, usando kNN
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE,
summaryFunction = twoClassSummary)
algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')
set.seed(100)
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList, metric = "ROC")
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList, metric = "ROC")
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList, metric = "ROC")
library(caret)
library(caretEnsemble)
orange <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/orange_juice_withmissing.csv')
# Create the training and test datasets
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(orange$Purchase, p=0.8, list=FALSE)
# Step 2: Create the training  dataset
trainData <- orange[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- orange[-trainRowNumbers,]
# Store X and Y for later use.
x = trainData[, 2:18]
y = trainData$Purchase
# Processo para preencher os dados vazios, NAs, usando kNN
preProcess_missingdata_model <- preProcess(trainData, method='knnImpute')
preProcess_missingdata_model
library(RANN)  # required for knnInpute
trainData <- predict(preProcess_missingdata_model, newdata = trainData)
anyNA(trainData)
# Stacking Algorithms - Run multiple algos in one call.
trainControl <- trainControl(method="repeatedcv",
number=10,
repeats=3,
savePredictions=TRUE,
classProbs=TRUE,
summaryFunction = twoClassSummary)
algorithmList <- c('rf', 'adaboost', 'earth', 'xgbDART', 'svmRadial')
set.seed(100)
models <- caretList(Purchase ~ ., data=trainData, trControl=trainControl, methodList=algorithmList, metric = "ROC")
library(C50) # algoritmo C5.0
library(modeldata) # dados do churn
library(caret)
library(pROC) # curva ROC
library(doParallel) # processa em paralelo
library(gbm) # algoritmo usado
allData <- mlc_churn # dados de churn Telecom
